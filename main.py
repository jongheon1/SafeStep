from fastapi import FastAPI
from fastapi import Body, Form, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
import json
import nest_asyncio
from pyngrok import ngrok
import uvicorn
from pydantic import BaseModel
from langchain_community.document_loaders import PyPDFDirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma
import os
from langchain_openai import ChatOpenAI
from langchain.schema.runnable import RunnableMap
from langchain.prompts import PromptTemplate
from langchain.callbacks.base import BaseCallbackHandler
from langchain.schema import Document
from dotenv import load_dotenv
from langchain_community.document_loaders import PyMuPDFLoader

load_dotenv()
persist_directory = os.getenv('PERSIST_DIRECTORY')
collection_name = os.getenv('COLLECTION_NAME')
openai_api_key = os.getenv('OPENAI_API_KEY')

class StreamHandler(BaseCallbackHandler):
    def __init__(self, container, initial_text=""):
        self.container = container
        self.text = initial_text

    def on_llm_new_token(self, token: str, **kwargs):
        self.text += token
        self.container.markdown(self.text + "â–Œ")

def setup_chroma(persist_directory, collection_name, openai_api_key):
    # OpenAIEmbeddings ê°ì²´ ìƒì„±
    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)

    # Chroma ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
    db = Chroma(
        collection_name=collection_name,
        embedding_function=embeddings,
        persist_directory=persist_directory
    )

    return db

def get_retriever(db):
    # Chroma ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ retriever ê°€ì ¸ì˜¤ê¸°
    retriever = db.as_retriever(search_kwargs={"k": 5})
    return retriever

def load_and_store_pdf(db, chunk_size=1000, chunk_overlap=200):
    # PDF íŒŒì¼ ë¡œë“œ
    loader = PyPDFDirectoryLoader('./')
    pages = loader.load_and_split()

    # í…ìŠ¤íŠ¸ ë¶„í• 
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap
    )
    texts = text_splitter.split_documents(pages)

    # Chroma ë°ì´í„°ë² ì´ìŠ¤ì— í…ìŠ¤íŠ¸ ì‚½ì…
    db.add_documents(texts)

    print(f"Loaded and stored {len(texts)} chunks")

def upload_pdf(db, uploaded_file):
    if uploaded_file is not None:

        filepath = "./uploaded/" + uploaded_file.filename
        with open(filepath, 'wb') as f:
            f.write(uploaded_file.getbuffer())
        
        loader = PyPDFDirectoryLoader(filepath)
        pages = loader.load_and_split()

        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=50
        )
        texts = text_splitter.split_documents(pages)

        # Chroma ë°ì´í„°ë² ì´ìŠ¤ì— í…ìŠ¤íŠ¸ ì‚½ì…
        db.add_documents(texts)

        print(f"Loaded and stored {len(texts)} chunks")

async def convert_pdf_to_text(uploaded_file):
    # PDF íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
    with open("temp.pdf", "wb") as f:
        f.write(uploaded_file.read())
    
    loader = PyMuPDFLoader("temp.pdf")
    documents = loader.load()
    text = "\n".join(doc.page_content for doc in documents)
    return text


def get_file_title(file_path):
    base_name = os.path.basename(file_path)  # íŒŒì¼ ì´ë¦„ê³¼ í™•ì¥ì ì¶”ì¶œ
    file_title = os.path.splitext(base_name)[0]  # í™•ì¥ì ì œê±°
    return file_title
     

db = setup_chroma(persist_directory, collection_name, openai_api_key)

# PDF íŒŒì¼ ë¡œë“œ ë° Chroma ë°ì´í„°ë² ì´ìŠ¤ì— ì‚½ì…
# load_and_store_pdf(db)

# Retriever ê°€ì ¸ì˜¤ê¸°
retriever = get_retriever(db)

# ChatOpenAI ëª¨ë¸ ì„¤ì •
chat_model = ChatOpenAI(openai_api_key=openai_api_key, model_name="gpt-3.5-turbo")

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •
template = """
[ChatBot Prompt]
ë‹¹ì‹ ì€ SafeStepì´ë¼ëŠ” ë²•ë¥  ìƒë‹´ ì±—ë´‡ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì‚¬íšŒì´ˆë…„ìƒë“¤ì´ ê·¼ë¡œ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆë„ë¡ ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ ì¡°ì–¸ì„ ì œê³µí•˜ëŠ” ê²ƒì´ ë‹¹ì‹ ì˜ ì—­í• ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•  ë•ŒëŠ” ë‹¤ìŒ ì‚¬í•­ì„ ê³ ë ¤í•˜ì„¸ìš”.

1. ê³µê°ê³¼ ìœ„ë¡œì˜ ë§ì„ ì „í•˜ì„¸ìš”.
2. ê´€ë ¨ ë²•ë¥ ê³¼ ê³µê³µê¸°ê´€ì˜ ê°€ì´ë“œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”.
3. êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ í•´ê²° ë°©ì•ˆì„ ì œì‹œí•˜ì„¸ìš”.
4. ì‚¬ìš©ìì˜ ë¬¸ì œ í•´ê²° ì˜ì§€ë¥¼ ê²©ë ¤í•˜ê³ , í•„ìš”í•œ ì§€ì›ê³¼ ì¡°ì–¸ì„ ì•„ë¼ì§€ ë§ˆì„¸ìš”.
5. SafeStepì´ ì‚¬ìš©ìì˜ ë“ ë“ í•œ ì¡°ë ¥ìì„ì„ ê°•ì¡°í•˜ì„¸ìš”.

[Question]
{question}

[Context]
{context}

[Assistant Answer]
{question}ì— ëŒ€í•´ [ê´€ë ¨ ë²•ë¥  ë° ì¡°í•­]ê³¼ [ê³µê³µê¸°ê´€ ê°€ì´ë“œ]ë¥¼ ì°¸ê³ í•˜ì—¬ ì•ˆë‚´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤.
ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ì„œëŠ” ë‹¤ìŒ ì ˆì°¨ë¥¼ ë”°ë¼ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤:

1.
2.
3.

...

ì ˆì°¨ ì§„í–‰ ì¤‘ ì–´ë ¤ì›€ì´ ìˆìœ¼ì‹œë©´, [ê´€ë ¨ ê¸°ê´€]ì— ë„ì›€ì„ ìš”ì²­í•˜ì„¸ìš”.
ì¦ê±° ìë£Œ ì¤€ë¹„ë„ ìŠì§€ ë§ˆì‹œê³ , íŠ¹íˆ [ì¤‘ìš” ì¦ê±° ìë£Œ]ë¥¼ ê¼­ í™•ë³´í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.
ë” ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë¬¸ì˜í•´ ì£¼ì„¸ìš”. SafeStepì´ í•­ìƒ í•¨ê»˜ í•˜ê² ìŠµë‹ˆë‹¤.
"""

prompt = PromptTemplate(template=template, input_variables=["question", "context"])

inputs = RunnableMap({
    'context': lambda x: retriever.get_relevant_documents(x['question']),
    'question': lambda x: x['question']
})
chain = inputs | prompt | chat_model

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=['*'],
    allow_credentials=True,
    allow_methods=['*'],
    allow_headers=['*'],
)


@app.post("/ask")
async def ask_question(question: str = Form(...), file: UploadFile = File(None)):
    pdf_text = ""
    if file is not None:
        pdf_text = convert_pdf_to_text(file)
    combined_question = question + "\n" + pdf_text
    response = chain.invoke({'question': question})
    answer = response.content
    source = retriever.get_relevant_documents(question)

    return {
        "answer": answer,
        "source": source
    }

if __name__ == "__main__":
    # ngrok_tunnel = ngrok.connect(8000)
    # print('Public URL:', ngrok_tunnel.public_url)
    # nest_asyncio.apply()
    uvicorn.run(app, host="0.0.0.0", port=8000)


# # Streamlit ì•±
# if 'messages' not in st.session_state:
#     st.session_state.messages = []

# # ì œëª©ê³¼ ì†Œê°œ
# st.title("SafeStep - ë‹¹ì‹ ì˜ ì•ˆì „í•œ ì²« ê±¸ìŒì„ ë„ì™€ë“œë¦½ë‹ˆë‹¤!")
# st.markdown("""
# **ê·¼ë¡œ ì‹œê°„ ê´€ë ¨ ë¬¸ì œë¡œ ì–´ë ¤ì›€ì„ ê²ªê³  ê³„ì‹ ê°€ìš”?**  
# ì €í¬ SafeStep ì±—ë´‡ì´ ë„ì™€ë“œë¦½ë‹ˆë‹¤! ì£¼ 52ì‹œê°„ì œ, ì´ˆê³¼ ê·¼ë¬´ ìˆ˜ë‹¹ ë¯¸ì§€ê¸‰, í¬ê´„ì„ê¸ˆì œ ë¬¸ì œ ë“± ë‹¤ì–‘í•œ ê·¼ë¡œ ì‹œê°„ ê´€ë ¨ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ë²•ì  ì¡°ì–¸ê³¼ ê°€ì´ë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤.  
# **ì§€ê¸ˆ ë°”ë¡œ ì§ˆë¬¸í•´ë³´ì„¸ìš”!** ìš°ë¦¬ì˜ AI ìƒë‹´ì›ì´ ì‹ ì†í•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ë“œë¦½ë‹ˆë‹¤.
# """)

# # PDF íŒŒì¼ ì—…ë¡œë“œ ì„¹ì…˜
# uploaded_file = st.file_uploader("ë¬¸ì œë¥¼ ê²ªê³  ìˆëŠ” ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš” ğŸ“„", type="pdf")

# if uploaded_file is not None:
#     upload_pdf(db, uploaded_file)
#     st.success("íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤. ğŸ”¥ ì´ì œ ì§ˆë¬¸ì„ ì…ë ¥í•´ë³´ì„¸ìš”!")

# # ì±„íŒ… ë©”ì‹œì§€ í‘œì‹œ
# for message in st.session_state.messages:
#     st.chat_message(message['role']).markdown(message['content'])

# # ì§ˆë¬¸ ì…ë ¥ ì„¹ì…˜
# if question := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš” ğŸ“:"):
#     st.session_state.messages.append({"role": "human", "content": question})

#     with st.chat_message('human'):
#         st.markdown(question)

#     with st.chat_message('assistant'):
#         response_placeholder = st.empty()

#     response = chain.invoke({'question': question}, config={'callbacks': [StreamHandler(response_placeholder)]})
#     answer = response.content

#     st.session_state.messages.append({"role": "ai", "content": answer})
#     response_placeholder.markdown(answer)

#     docs = retriever.get_relevant_documents(question)
#     if docs:
#         with st.expander("ğŸ” ì°¸ê³  ìë£Œ í™•ì¸"):
#             for i, doc in enumerate(docs, start=1):
#                 st.markdown(f"##### ì¶œì²˜ {i}")
#                 st.markdown(f"- {get_file_title(doc.metadata['source'])} / {doc.metadata['page']}p")
#                 st.markdown("##### ë‚´ìš©")
#                 st.markdown(doc.page_content)
#                 st.markdown("---")


# # ì…ë ¥ê°’ì— ëŒ€í•œ ë‹µë³€ ìƒì„±
# while True:
#     query = input("Ask a question (or type 'exit' to quit): ")
#     if query.lower() == "exit":
#         break

#     response = chain.invoke({'question': query})
#     answer = response.content

#     print(answer)

#     docs = retriever.get_relevant_documents(query)

#     sources = "\nSources:\n"
#     for doc in docs:
#         sources += f"- {doc.metadata['source']} {doc.metadata['page']} page\n"
#         sources += f"  Content: {doc.page_content}\n"

#     print(sources)
