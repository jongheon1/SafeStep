import streamlit as st
from langchain_community.document_loaders import PyPDFDirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma
import os
from langchain_openai import ChatOpenAI
from langchain.schema.runnable import RunnableMap
from langchain.prompts import PromptTemplate
from langchain.callbacks.base import BaseCallbackHandler
from dotenv import load_dotenv
from langchain_community.document_loaders import PyMuPDFLoader


load_dotenv()
persist_directory = os.getenv('PERSIST_DIRECTORY')
collection_name = os.getenv('COLLECTION_NAME')
openai_api_key = os.getenv('OPENAI_API_KEY')

class StreamHandler(BaseCallbackHandler):
    def __init__(self, container, initial_text=""):
        self.container = container
        self.text = initial_text

    def on_llm_new_token(self, token: str, **kwargs):
        self.text += token
        self.container.markdown(self.text + "â–Œ")

def setup_chroma(persist_directory, collection_name, openai_api_key):
    # OpenAIEmbeddings ê°ì²´ ìƒì„±
    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)

    # Chroma ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
    db = Chroma(
        collection_name=collection_name,
        embedding_function=embeddings,
        persist_directory=persist_directory
    )

    return db

def get_retriever(db):
    # Chroma ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ retriever ê°€ì ¸ì˜¤ê¸°
    retriever = db.as_retriever(search_kwargs={"k": 5})
    return retriever

def load_and_store_pdf(db, chunk_size=1000, chunk_overlap=50):
    # PDF íŒŒì¼ ë¡œë“œ
    loader = PyPDFDirectoryLoader('./')
    pages = loader.load_and_split()

    # í…ìŠ¤íŠ¸ ë¶„í• 
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap
    )
    texts = text_splitter.split_documents(pages)

    # Chroma ë°ì´í„°ë² ì´ìŠ¤ì— í…ìŠ¤íŠ¸ ì‚½ì…
    db.add_documents(texts)

    print(f"Loaded and stored {len(texts)} chunks")

def upload_pdf(db, uploaded_file):
    if uploaded_file is not None:

        filepath = "./uploaded/" + uploaded_file.name
        with open(filepath, 'wb') as f:
            f.write(uploaded_file.getbuffer())
        
        loader = PyPDFDirectoryLoader(filepath)
        pages = loader.load_and_split()

        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=50
        )
        texts = text_splitter.split_documents(pages)

        # Chroma ë°ì´í„°ë² ì´ìŠ¤ì— í…ìŠ¤íŠ¸ ì‚½ì…
        db.add_documents(texts)

        print(f"Loaded and stored {len(texts)} chunks")

def convert_pdf_to_text(uploaded_file):
    # PDF íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
    with open("temp.pdf", "wb") as f:
        f.write(uploaded_file.read())
    
    loader = PyMuPDFLoader("temp.pdf")
    documents = loader.load()
    text = "\n".join(doc.page_content for doc in documents)
    return text


def get_file_title(file_path):
    base_name = os.path.basename(file_path)  # íŒŒì¼ ì´ë¦„ê³¼ í™•ì¥ì ì¶”ì¶œ
    file_title = os.path.splitext(base_name)[0]  # í™•ì¥ì ì œê±°
    return file_title


db = setup_chroma(persist_directory, collection_name, openai_api_key)

# PDF íŒŒì¼ ë¡œë“œ ë° Chroma ë°ì´í„°ë² ì´ìŠ¤ì— ì‚½ì…
# load_and_store_pdf(db)

# Retriever ê°€ì ¸ì˜¤ê¸°
retriever = get_retriever(db)

# ChatOpenAI ëª¨ë¸ ì„¤ì •
chat_model = ChatOpenAI(openai_api_key=openai_api_key, model_name="gpt-3.5-turbo")

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •
template = """
[ChatBot Prompt]
ë‹¹ì‹ ì€ SafeStepì´ë¼ëŠ” ë²•ë¥  ìƒë‹´ ì±—ë´‡ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì‚¬íšŒì´ˆë…„ìƒë“¤ì´ ê·¼ë¡œ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆë„ë¡ ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ ì¡°ì–¸ì„ ì œê³µí•˜ëŠ” ê²ƒì´ ë‹¹ì‹ ì˜ ì—­í• ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•  ë•ŒëŠ” ë‹¤ìŒ ì‚¬í•­ì„ ê³ ë ¤í•˜ì„¸ìš”.

1. ê³µê°ê³¼ ìœ„ë¡œì˜ ë§ì„ ì „í•˜ì„¸ìš”.
2. ê´€ë ¨ ë²•ë¥ ê³¼ ê³µê³µê¸°ê´€ì˜ ê°€ì´ë“œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”.
3. êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ í•´ê²° ë°©ì•ˆì„ ì œì‹œí•˜ì„¸ìš”.
4. ì‚¬ìš©ìì˜ ë¬¸ì œ í•´ê²° ì˜ì§€ë¥¼ ê²©ë ¤í•˜ê³ , í•„ìš”í•œ ì§€ì›ê³¼ ì¡°ì–¸ì„ ì•„ë¼ì§€ ë§ˆì„¸ìš”.
5. SafeStepì´ ì‚¬ìš©ìì˜ ë“ ë“ í•œ ì¡°ë ¥ìì„ì„ ê°•ì¡°í•˜ì„¸ìš”.

[Question]
{question}

[Context]
{context}

[Assistant Answer]
{question}ì— ëŒ€í•´ [ê´€ë ¨ ë²•ë¥  ë° ì¡°í•­]ê³¼ [ê³µê³µê¸°ê´€ ê°€ì´ë“œ]ë¥¼ ì°¸ê³ í•˜ì—¬ ì•ˆë‚´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤.
ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ì„œëŠ” ë‹¤ìŒ ì ˆì°¨ë¥¼ ë”°ë¼ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤:

1.
2.
3.

...

ì ˆì°¨ ì§„í–‰ ì¤‘ ì–´ë ¤ì›€ì´ ìˆìœ¼ì‹œë©´, [ê´€ë ¨ ê¸°ê´€]ì— ë„ì›€ì„ ìš”ì²­í•˜ì„¸ìš”.
ì¦ê±° ìë£Œ ì¤€ë¹„ë„ ìŠì§€ ë§ˆì‹œê³ , íŠ¹íˆ [ì¤‘ìš” ì¦ê±° ìë£Œ]ë¥¼ ê¼­ í™•ë³´í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.
ë” ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë¬¸ì˜í•´ ì£¼ì„¸ìš”. SafeStepì´ í•­ìƒ í•¨ê»˜ í•˜ê² ìŠµë‹ˆë‹¤.
"""

prompt = PromptTemplate(template=template, input_variables=["question", "context"])

inputs = RunnableMap({
    'context': lambda x: retriever.invoke(x['question']),
    'question': lambda x: x['question']
})
chain = inputs | prompt | chat_model



# Streamlit ì•±

if 'messages' not in st.session_state:
    st.session_state.messages = []



st.title("SafeStep")
st.subheader("ë‹¹ì‹ ì˜ ì•ˆì „í•œ ì²« ê±¸ìŒì„ ë„ì™€ë“œë¦½ë‹ˆë‹¤")

# PDF íŒŒì¼ ì—…ë¡œë“œ ì„¹ì…˜
uploaded_file = st.file_uploader("ê²ªê³  ìˆëŠ” ë¬¸ì œì™€ ê´€ë ¨ìˆëŠ” ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš” ğŸ“„", type="pdf")

if uploaded_file is not None:
    st.success("íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤. ğŸ”¥ ì´ì œ ì§ˆë¬¸ì„ ì…ë ¥í•´ë³´ì„¸ìš”!")

# ì±„íŒ… ë©”ì‹œì§€ í‘œì‹œ
for message in st.session_state.messages:
    st.chat_message(message['role']).markdown(message['content'])

example_prompts1 = [
    "ë§¤ì£¼ 60ì‹œê°„ ì´ìƒ ê·¼ë¬´í•˜ê³  ìˆì§€ë§Œ, ì´ˆê³¼ ìˆ˜ë‹¹ì„ ë°›ì§€ ëª»í•˜ê³  ìˆì–´ìš”. ì–´ë–»ê²Œ í•´ì•¼ í• ê¹Œìš”?",
    "ìˆ˜ìŠµ ê¸°ê°„ ì¤‘ì¸ë°, ì •ë‹¹í•œ ì´ìœ  ì—†ì´ ê°‘ìê¸° í•´ê³  í†µë³´ë¥¼ ë°›ì•˜ìŠµë‹ˆë‹¤. ì´ëŸ´ ë• ì–´ë–¤ ì¡°ì¹˜ë¥¼ ì·¨í•´ì•¼ í•˜ë‚˜ìš”?",
    "í¬ê´„ì„ê¸ˆì œ ê³„ì•½ì„œì— ì‚¬ì¸í–ˆëŠ”ë°, ê³¼ë„í•œ ì•¼ê·¼ì— ì‹œë‹¬ë¦¬ê³  ìˆìŠµë‹ˆë‹¤. ê³„ì•½ì„œ ë‚´ìš©ì´ ë¶€ë‹¹í•˜ë‹¤ê³  ëŠê»´ì§€ëŠ”ë° ì–´ë–»ê²Œ ëŒ€ì‘í•´ì•¼ í• ì§€ ëª¨ë¥´ê² ì–´ìš”."
]
example_prompts2 = [
    "3ê°œì›” ì§¸ ì„ê¸ˆì„ ë°›ì§€ ëª»í•˜ê³  ìˆìŠµë‹ˆë‹¤. íšŒì‚¬ê°€ ê³„ì† ë¯¸ë£¹ë‹ˆë‹¤. ì œ ê¶Œë¦¬ë¥¼ ì§€í‚¤ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ í•˜ì£ ?",
    "ì£¼ 52ì‹œê°„ì œê°€ ì‹œí–‰ ì¤‘ì¸ë°, íšŒì‚¬ì—ì„œ ì´ˆê³¼ ê·¼ë¬´ë¥¼ ê°•ìš”í•˜ê³  ìˆìŠµë‹ˆë‹¤. ê±°ì ˆí•˜ë©´ ë¶ˆì´ìµì„ ì¤„ ê²ƒ ê°™ì•„ ë‘ë µìŠµë‹ˆë‹¤. ì¡°ì–¸ ë¶€íƒë“œë ¤ìš”.",
    "í‡´ì§ í›„ì—ë„ ì„ê¸ˆ ì²´ë¶ˆ ë¬¸ì œë¡œ ê³ ìƒ ì¤‘ì…ë‹ˆë‹¤. íšŒì‚¬ê°€ ê³„ì† ì§€ê¸‰ì„ ë¯¸ë£¨ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŸ´ ë• ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?"
]

button_cols1 = st.columns(3)
button_cols2 = st.columns(3)

button_pressed = ""

for i, prompt in enumerate(example_prompts1):
    if button_cols1[i].button(prompt):
        button_pressed = prompt

for i, prompt in enumerate(example_prompts2):
    if button_cols2[i].button(prompt):
        button_pressed = prompt

# ì§ˆë¬¸ ì…ë ¥ ì„¹ì…˜
if question := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš” ğŸ“:") or button_pressed:
    st.session_state.messages.append({"role": "human", "content": question})

    with st.chat_message('human'):
        st.markdown(question)

    with st.chat_message('assistant'):
        response_placeholder = st.empty()

    pdf_text = ""
    if uploaded_file is not None:
        pdf_text = convert_pdf_to_text(uploaded_file)
    combined_question = question + "\n" + pdf_text

    response = chain.invoke({'question': combined_question}, config={'callbacks': [StreamHandler(response_placeholder)]})
    answer = response.content

    st.session_state.messages.append({"role": "ai", "content": answer})
    response_placeholder.markdown(answer)

    docs = retriever.invoke(question)
    if docs:
        with st.expander("ğŸ” ì°¸ê³  ìë£Œ í™•ì¸"):
            for i, doc in enumerate(docs, start=1):
                st.markdown(f"##### ì¶œì²˜ {i}")
                st.markdown(f"- {get_file_title(doc.metadata['source'])} / {doc.metadata['page']}p")
                st.markdown("##### ë‚´ìš©")
                st.markdown(doc.page_content)
                st.markdown("---")# ì§ˆë¬¸ ì…ë ¥ ì„¹ì…˜
                